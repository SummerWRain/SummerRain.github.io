<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>扩散模型总结</title>
    <link href="/2023/12/08/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/"/>
    <url>/2023/12/08/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h1 id="扩散模型总结">扩散模型总结</h1><blockquote><p>最近在回顾之前写的Understanding DiffusionModels时，发现有个问题，文章太细了，每一步都是数学推导，于是，这篇对其进行一次总结，使得对模型理解一目了然。</p></blockquote><h2 id="前向过程">前向过程</h2><p><span class="math display">\[q(x_t|x_{t-1})=\mathcal N(x_t;\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I)\]</span></p><p><span class="math inline">\(x_0\sim q(x_0)\)</span>,<spanclass="math inline">\(\beta_1,...,\beta_T\)</span> is the varianceschedule, <span class="math inline">\(\alpha_t=1-\beta_t\)</span><br /><span class="math display">\[q(x_t|x_0)=\mathcal N(x_t;\sqrt{\bar\alpha_t}x_0,(1-\bar\alpha_t)I)\]</span> <spanclass="math inline">\(\bar\alpha_t=\prod_{s=1}^t\alpha_s\)</span> .</p><h2 id="反向过程">反向过程</h2><p><span class="math display">\[q(x_{t-1}|x_t,x_0)=\mathcal N(x_{t-1};\mu_q(x_t,x_0),\Sigma_q(t))\]</span></p><p><spanclass="math inline">\(\mu_q(x_t,x_0)=\frac{\sqrt\alpha_t(1-\bar\alpha_{t-1})x_t+\sqrt{\bar\alpha_{t-1}}(1-\alpha_t)x_0}{1-\bar\alpha_t}\)</span><span class="math display">\[p_\theta(x_{t-1}|x_t)=\mathcalN(x_{t-1};\mu_\theta(x_t,t),\Sigma_\theta(x_t,t))\]</span></p><h2 id="损失">损失</h2><p><span class="math display">\[L_{simple}(\theta) = \mathbbE_{t,x_0,\epsilon}[\left\|\epsilon-\epsilon_\theta(x_t,t)\right\|^2]\]</span></p><p><span class="math inline">\(x_t\sim q(x_t|x_0)\)</span></p><h2 id="采样">采样</h2><p>。。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Diffusion</tag>
      
      <tag>Code</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随机过程</title>
    <link href="/2023/12/07/%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B/"/>
    <url>/2023/12/07/%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="随机过程">随机过程</h1><h2 id="brown运动wiener过程">Brown运动（Wiener过程）</h2><p>若随机过程 <span class="math inline">\(W(t)\)</span>满足以下三条性质：</p><ol type="1"><li><strong>轨线连续</strong> <span class="math inline">\(W(0)=0,W(t)是关于 t 的连续函数\)</span></li><li><strong>增量正态分布</strong> 对于固定 <spanclass="math inline">\(t\)</span> 有 <span class="math inline">\(W(t)\sim\mathcal N(0,t)\)</span>，以及 <span class="math inline">\(W(t)-W(s)\sim\mathcal N(0,t-s)\)</span></li><li><strong>增量独立</strong> <spanclass="math inline">\(W(t_n)-W(t_{n-1}),...,W(t_2)-W(t_1)\)</span> 与<span class="math inline">\(W(t_1)\)</span> 之间相互独立</li></ol><p>则我们称 <span class="math inline">\(W(t)\)</span>为<strong>Brown运动</strong>或<strong>Wiener过程</strong></p><h3 id="二次变差">二次变差</h3><p><span class="math inline">\(\mathbb E[W(t)]^2=t\iff\int_0^t(dW)^2=\int_0^tdt\iff (dW)^2=dt\)</span></p><blockquote><p>proof:</p><p><spanclass="math inline">\(将[0,t]划分成n份\to(0,\frac{t}{n},\frac{2n}{t},...,t),记增量\[W(\frac{k}{n}t)-W(\frac{k-1}{n}t)]=\Delta W(\frac{k}{n})\)</span></p><p>等价于证明 <span class="math inline">\(\sum\limits_{k=1}^n[\DeltaW(\frac{k}{n})]^2\xrightarrow[n\to\infty]{m.s}t\)</span>，即 <spanclass="math inline">\(E[\sum\limits_{k=1}^n(\DeltaW(\frac{k}{n}))^2-t]^2\xrightarrow[n\to\infty]{m.s}0\)</span></p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204826498.png" /></p><p>命题得证</p></blockquote><h2 id="itos-formula">Ito’s formula</h2><blockquote><p><span class="math display">\[dg(t,W(t))=\frac{\partial g}{\partial t}dt+\frac{\partial g}{\partialW}dW+\frac12\frac{\partial^2g}{\partial W^2}dt,\\because\ (dW)^2=dt.\]</span></p></blockquote><h3 id="ir-sde应用">IR-SDE应用</h3><p>假设一个 <span class="math inline">\(x\)</span> 是一个随机过程：<span class="math display">\[dx=Fdt+Gd\omega\]</span> 想要估计 <span class="math inline">\(\psi(x,t)\)</span>,使用泰勒展开： <span class="math display">\[\psi(x+\Delta x,t+\Delta t)\\=\psi(x,t)+\frac{\partial\psi}{\partial x}\Deltax+\frac{\partial\psi}{\partial t}\Delta t+\\\frac12\big[\frac{\partial^2\psi}{\partial x^2}\Deltax^2+\frac{\partial^2\psi}{\partial x\partial t}\Delta t\Deltax+\frac{\partial^2\psi}{\partial t^2}(\Delta t)^2\big]\]</span> 当 <span class="math inline">\(\Delta t\to0^+\)</span> 时：<span class="math display">\[d\psi(x,t)=\frac{\partial\psi}{\partialx}dx+\frac{\partial\psi}{\partialt}dt+\frac12\frac{\partial^2\psi}{\partial x^2}dx^2\\=\frac{\partial\psi}{\partial t}dt+\frac{\partial\psi}{\partialx_t}(Fdt+Gd\omega)+\frac12\frac{\partial^2\psi}{\partialx^2}(Fdt+Gd\omega)^2\\=(\frac{\partial\psi}{\partial t}+\frac{\partial\psi}{\partialx_t}F+\frac12G^2\frac{\partial^2\psi}{\partialx^2})dt+G\frac{\partial\psi}{\partial x}d\omega\]</span></p>]]></content>
    
    
    
    <tags>
      
      <tag>math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Diffusion GAN</title>
    <link href="/2023/12/05/Diffusion%20GAN/"/>
    <url>/2023/12/05/Diffusion%20GAN/</url>
    
    <content type="html"><![CDATA[<h1id="tackling-the-generative-learning-trilemma-with-denoising-diffusion-gans">Tacklingthe Generative Learning Trilemma with Denoising Diffusion GANs</h1><h2 id="背景">背景</h2><p>生成式学习三困境：高质量采样，多样性，快速采样</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207203928494.png" /></p><p>传统扩散每一步的高斯假设只在去噪步长非常小的时候成立，因此反向过程需要很多步，而反向过程中步长较大（更少的去噪步数）时，需要使用非高斯多峰分布来建模去噪分布。</p><h2 id="贡献">贡献</h2><ol type="1"><li>将扩散模型缓慢采样的原因归结为去噪分布的高斯假设，并提出使用复杂、多模的去噪分布</li><li>提出 Denoising Diffusion GANs，使用 conditionalGANs参数化扩散模型的反向过程</li></ol><h2 id="方法">方法</h2>]]></content>
    
    
    
    <tags>
      
      <tag>Diffusion</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ELITE</title>
    <link href="/2023/11/28/ELITE/"/>
    <url>/2023/11/28/ELITE/</url>
    
    <content type="html"><![CDATA[<h1id="elite-encoding-visual-concepts-into-textual-embeddings-for-customized-text-to-image-generation">ELITE:Encoding Visual Concepts into Textual Embeddings for CustomizedText-to-Image Generation</h1><h2 id="背景">背景</h2><p>customized文本到图像的生成问题：<strong>由于用户可能会使用难以形容的、个人的一些概念去创造一些有想象力的样本</strong>（例：“柯基”）。通常需要从用户提供的少量图片集合去学习特定的概念，现存工作使用一种基于优化的方法来学习customized 概念，但是会带来过多的计算和内存负担。</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207203952840.png" /></p><p>基于GAN和VAE的方法不能很好的匹配用户的描述，大文生图模型也难以表达特定的或用户定义的概念。</p><h2 id="贡献">贡献</h2><p>为快速准确的 customized文本到图像生成提出基于学习的编码器ELITE，由全局-局部映射网络组成，直接将视觉概念编码为文本嵌入。在将学习到的概念编辑到一个新的场景中时有较好的灵活性，同时保留图像特有的细节。</p><p>全局映射网络将给定图像的层级特征投射到文本的 word embedding空间中的多个“新”的单词中（例：一个主词表示可编辑的概念，以及其他辅助词来排除不相关的干扰）；局部映射网络将编码后的patch 特征注入到交叉注意力层以提供遗漏的细节。</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204007671.png" /></p><h2 id="方法">方法</h2><h3 id="预备">预备</h3><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204021828.png" /></p><p>使用 Stable diffusion作为文生图模型：首先训练一个autoencoder，encoder <spanclass="math inline">\(z=\varepsilon(x)\)</span>将一张图像映射到低维空间，同时decoder <spanclass="math inline">\(D(\varepsilon(x))\approx x\)</span>将隐编码还原为一张图像;然后在隐空间上训练条件扩散模型 <spanclass="math inline">\(\epsilon_\theta(\cdot)\)</span> ，基于条件 <spanclass="math inline">\(y\)</span> 生成隐编码。使用均方误差来训练： <spanclass="math display">\[L_{LDM}=\mathbb E_{z\sim\varepsilon(x),y,\epsilon\sim\mathcalN(0,1),t}\big[\left\|\epsilon-\epsilon_\theta(z_t,t,\tau_\theta(y))\right\|_2^2\big]\]</span> <span class="math inline">\(\tau_\theta(\cdot)\)</span>表示CLIP text-encoder。推理时，高斯噪声 <spanclass="math inline">\(z_T\)</span> 逐渐去噪为 <spanclass="math inline">\(z_0\)</span> ,然后最终的图像通过decoder得到 <spanclass="math inline">\(x&#39;=\mathcal D(z_0)\)</span></p><p><strong>交叉注意力</strong> 为了在生成过程中利用文本信息</p><blockquote><p><span class="math display">\[Attention(Q,K,V)=Softmax(\frac{QK^T}{\sqrt{d&#39;}})V\]</span></p><p><span class="math inline">\(Q=W_Q\cdot f,K=W_K\cdot\tau_\theta(y),V=W_V\cdot\tau_\theta(y)\)</span> ，<spanclass="math inline">\(f\)</span> 为隐图像特征，<spanclass="math inline">\(\tau_\theta(y)\)</span> 为文本特征，<spanclass="math inline">\(d&#39;\)</span>为key和query的输出维度，隐图像特征通过注意力块更新。</p></blockquote><hr /><h3 id="全局映射网络"><strong>全局映射网络</strong></h3><p>使用CLIP图像编码器 <spanclass="math inline">\(\psi_\theta(\cdot)\)</span>作为特征提取器，全局映射网络 <spanclass="math inline">\(M^g(\cdot)\)</span>特征投影到CLIP文本编码器的文本词嵌入<span class="math inline">\(v\)</span> : <span class="math display">\[v=M^g\circ\psi_\theta(x)\]</span> <span class="math inline">\(v\in \mathbb R^{N\timesd}\)</span>，<span class="math inline">\(N\)</span> 为单词数，<spanclass="math inline">\(d\)</span>为词向量的维度，对特征使用全局平均池化来获得词嵌入。由于一张图像会有主体和其他不相关元素，编码为单个单词会损失主体的可编辑性，于是分别学习主词和辅助词：从CLIP最深层特征学到主要概念（主体），从其他层学到的辅助词描述其他不相关元素。从CLIP图像编码器中选择<span class="math inline">\(N\)</span> 层，每一层 <spanclass="math inline">\(\psi_\theta^{L_i}(\cdot)\)</span>独立的学习一个单词 <span class="math inline">\(w_i\)</span></p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204032657.png" /></p><p>训练目标： <span class="math display">\[L_{global}=L_{LDM}+\lambda_{global}\left\|v\right\|_1\]</span> 训练时随机从CLIP ImageNettemplates采样一个文本作为文本输入，key 和 value 分别通过 <spanclass="math inline">\(M^g(\cdot)\)</span> 微调。<spanclass="math inline">\(K^g=W_K^g\cdot\tau_\theta(y),V^g=W_V^g\cdot\tau_\theta(y)\)</span></p><hr /><h3 id="局部映射网络"><strong>局部映射网络</strong></h3><p>局部映射网络 <span class="math inline">\(M^l(\cdot)\)</span>将多层CLIP特征编码到文本特征空间中（文本编码器的输出空间）。 <spanclass="math display">\[e=M^l\circ\psi_\theta(x*m)\]</span> <span class="math inline">\(m\)</span>是对象的mask，用来回避背景中不需要的细节，<spanclass="math inline">\(e\in\mathbb R^{p\times p\times d}\)</span>保持空间结构，<span class="math inline">\(p\)</span> 为特征的size。<spanclass="math inline">\(e\)</span>的每个像素主要集中于给定图像的每个patch的局部细节，然后将得到的文本嵌入注入到交叉注意力层<spanclass="math inline">\(Attention(Q,K^l,V^l),K^l=W_K^l\cdot(e*m),V^l=W_V^l\cdot(e*m)\)</span>，与全局部分融合以改进局部细节:<span class="math display">\[Out=Attention(Q,K^g,V^g)+\lambda Attention(Q,K^l,V^l)\]</span> 为了强调目标区域，将得到的注意力图 <spanclass="math inline">\(Q{K^l}^T\)</span> 通过 <spanclass="math inline">\(Q{K^g}^T\)</span> 重新加权。</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204040735.png" /></p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204047752.png" /></p><p>训练目标： <span class="math display">\[L_{local}=L_{LDM}+\lambda_{local}\left\|V^l\right\|_1\]</span></p><hr /><h2 id="实验">实验</h2><p>使用OpenImage的测试集进行训练，主体mask通过预训练的分割模型得到。映射网络使用3层MLP，选择{24，4，8，12，16}层的CLIP特征，采样器使用LMS采样器。</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204055309.png" /></p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204103826.png" /></p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204110051.png" /></p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204126287.png" /></p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204133882.png" /></p><h2 id="局限">局限</h2><p>不能处理涉及文本字符的图片：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204146549.png" /></p>]]></content>
    
    
    
    <tags>
      
      <tag>Diffusion</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Understanding Diﬀusion Models</title>
    <link href="/2023/11/13/Understanding%20diffusion/"/>
    <url>/2023/11/13/Understanding%20diffusion/</url>
    
    <content type="html"><![CDATA[<h1id="understanding-diﬀusion-models-a-uniﬁed-perspective">UnderstandingDiﬀusion Models: A Uniﬁed Perspective</h1><h2 id="elbo">ELBO</h2><p>目标：学习一个模型去最大化 <span class="math inline">\(p(x)\)</span>的似然</p><p>法一：<span class="math inline">\(p(x)=\int p(x,z)dz\)</span></p><p>法二：<spanclass="math inline">\(p(x)=\frac{p(x,z)}{p(z|x)}\)</span></p><p>代理目标：<strong>最大化ELBO</strong></p><p><span class="math display">\[\log{p(x)} \ge \mathbbE_{q_\phi(z|x)}\Big[\log{\frac{p(x,z)}{q_\phi(z|x)}}\Big]\]</span></p><p>why？</p><p>使用法一证明：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204336825.png" /></p><p>并不能告诉我们太多幕后实际情况的有用信息，而且这个证明并没有直观的给出究竟为什么ELBO实际上是证据的下限</p><p>于是使用法二证明：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204345263.png" /></p><p>因此证据就等于ELBO加上KL散度</p><p><span class="math inline">\(\log{p(x)}\)</span> 对于 <spanclass="math inline">\(\phi\)</span> 总是一个常数，因此，<spanstyle="background-color: #ff666680">最大化ELBO项就相当于最小化KL散度项</span></p><h2 id="vae">VAE</h2><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204353735.png" /></p><p>输入数据经过中间的bottleneck表示步骤后被训练来预测它本身</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204400769.png" /></p><p><strong>encoder</strong>：学习中间的bottleneck分布 <spanclass="math inline">\(q_\phi(z|x)\)</span>将输入转换为可能的潜在分布</p><p><strong>decoder</strong>: 学习确定的函数 <spanclass="math inline">\(p_\theta(x|z)\)</span> 将输入的潜在向量 <spanclass="math inline">\(z\)</span> 转换为观测 <spanclass="math inline">\(x\)</span></p><p>第一项衡量变分分布的decoder的重建似然，保证学到的分布可以建模有效的可以重新生成原始数据的潜在变量；第二项衡量学习到的变分分布和潜在变量的先验信念有多相似，最小化该项可以避免学到的分布坍塌为Diracdelta函数</p><p><u>那么最大化ELBO等价于最大化第一项、最小化第二项</u></p><p>VAE的encoder通常通过对数协方差建模一个多元高斯，先验器通常被选择为一个标准高斯：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204408488.png" /></p><p>KL散度项在分析上可以被计算，重建项可以使用蒙特卡罗方法近似，于是目标变为：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204416402.png" /></p><p>由于每个 <span class="math inline">\(z\)</span>被随机采样，通常不可微，因此通过重新参数化技巧解决：重写随机变量为一个确定的噪声变量的函数。于是每个<span class="math inline">\(z\)</span> 可以被输入为 <spanclass="math inline">\(x\)</span> 的确定函数以及辅助噪声变量 <spanclass="math inline">\(\epsilon\)</span> 所计算：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204423652.png" /></p><p>训练VAE后，可以直接从隐空间 <span class="math inline">\(p(z)\)</span>采样，然后通过decoder生成新数据。当 <spanclass="math inline">\(z\)</span> 的维度小于 <spanclass="math inline">\(x\)</span>时，或许可以学习到重要、有趣的表示，此外，当语义上有意义的隐空间被学习，隐向量可以在通过decoder之前被编辑来控制数据的生成</p><h2 id="hierarchical-variational-autoencoders">Hierarchical VariationalAutoencoders</h2><p>一般的HVAE有T个层级，每个隐变量都可以以前面的隐变量为条件</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204430964.png" /></p><p>decoding每个 <span class="math inline">\(z_t\)</span> 都只以前面的<span class="math inline">\(z_{t+1}\)</span>为条件（可以理解为递归式的VAE），联合分布和后验器定义为：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204438090.png" /></p><p>扩展ELBO为：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204445749.png" /></p><p>代入联合分布、后验器：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204453280.png" /></p><p>变分扩散模型可以更进一步将这个目标分解为可解释的组分</p><h2 id="variational-diffusion-models">Variational Diffusion Models</h2><p>三个关键约束：</p><ul><li><p>隐变量维度与数据维度相同</p></li><li><p>每一步的隐变量编码器为一个预定义的线性高斯模型，不需要学习</p></li><li><p>最后的时间步T时刻隐变量为一个标准高斯</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204500602.png" /></p></li></ul><p>改写后验器：</p><p><span class="math display">\[q(x_{1:T}|x_0)=\prod_{t=1}^{T}q(x_t|x_{t-1})\tag {30}\]</span></p><p>encoder转移：</p><p><span class="math display">\[q(x_t|x_{t-1})=\mathcal N(x_t;\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)\mathbfI)\tag{31}\]</span></p><p>改写HVAE的联合分布：</p><p><span class="math display">\[p(x_{0:T})=p(x_T)\prod_{t=1}^Tp_\theta(x_{t-1|x_t})\tag{32}\]</span></p><p><span class="math display">\[p(x_T)=\mathcal N(x_T;0,\mathbf I)\tag{33}\]</span></p><blockquote><p>逐渐地添加噪声破坏图像，直至其最终与纯高斯噪声完全相同</p></blockquote><p>VDM的采样过程变为：从<span class="math inline">\(p(x_T)\)</span>采样一个高斯噪声，然后逐渐运行去噪转移 <spanclass="math inline">\(p_\theta(x_{t-1}|x_t)\)</span> 生成新的 <spanclass="math inline">\(x_0\)</span>。通过最大化ELBO来优化VDM：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204510858.png" /></p><blockquote><p><strong>重建项</strong>预测给定第一步隐变量后原始数据的对数似然；<strong>先验匹配项</strong>不需要被优化，假设一个足够大的T，最后的分布为高斯分布，这项实际上为0；<strong>一致项</strong>使得一张噪声图的去噪步应该匹配一张干净图的加噪步：</p></blockquote><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204518184.png" /></p><p>上面的结果是可能是次优解，由于一致项作为一个期望在每个时间步有两个随机变量，使用蒙特卡洛估计可能会有较高的方差。改进方案：<spanstyle="background-color: #ff666680">将encoder转移改写为 <spanclass="math"><spanclass="math inline">\(q(x_t|x_{t-1})=q(x_t|x_{t-1},x_0)\)</span></span>，</span>根据贝叶斯规则继续改写：</p><p><span class="math display">\[q(x_t|x_{t-1},x_0)=\frac{q(x_{t-1}|x_t,x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)}\tag{46}\]</span></p><p>重新进行推导ELBO：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204525454.png" /></p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204532467.png" /></p><blockquote><p><strong>重建项</strong>与普通的VAE相似，可以使用蒙特卡洛估计来近似；<strong>先验匹配项</strong>表示最终的噪声分布与标准高斯先验分布有多接近，同样的不需要训练，根据假设同样趋于0；<strong>去噪匹配项</strong>中的<span class="math inline">\(q(x_{t-1}|x_t,x_0)\)</span>转移步可以看作ground-truth信号，定义了有噪声的图像 <spanclass="math inline">\(x_t\)</span>应该如何去噪，并获得最后的完全去噪的图像 <spanclass="math inline">\(x_0\)</span>  应该是什么</p></blockquote><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204539891.png" /></p><p><strong>去噪匹配项</strong>中的KL散度难以计算，但是可以使用高斯转移的假设进行优化，根据贝叶斯规则：</p><p><span class="math display">\[q(x_{t-1}|x_t,x_0)=\frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)}\]</span></p><p><span class="math inline">\(q(x_t|x_{t-1})=q(x_t|x_{t-1})=\mathcalN(x_t;\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)\mathbf I)\)</span>，根据重新参数化技巧，重写采样 <span class="math inline">\(x_t\simq(x_t|x_{t-1})\)</span> :</p><p><span class="math display">\[x_t=\sqrt \alpha_tx_{t-1}+\sqrt{1-\alpha_t}\epsilon\tag{59}\]</span></p><p><span class="math display">\[x_{t-1}=\sqrt \alpha_{t-1}x_{t-2}+\sqrt{1-\alpha_{t-1}}\epsilon\tag{60}\]</span></p><p>于是：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204547855.png" /></p><p>代入得：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204554664.png" /></p><p>将方差重写，<spanclass="math inline">\(\Sigma_q(t)=\sigma_q^2(t)\mathbf I\)</span>,则：</p><p><span class="math display">\[\sigma_q^2(t)=\frac{(1-\alpha_t)(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}\tag{85}\]</span></p><p>为了匹配近似ground-truth去噪转移步<span style="color: #eb52f7"> <spanclass="math"><spanclass="math inline">\(q(x_{t-1}|x_t,x_0)\)</span></span></span>，将近似<span style="color: #4eb31c"> <span class="math"><spanclass="math inline">\(p_\theta(x_{t-1}|x_{t})\)</span></span></span>也建模为高斯分布。由于 <spanclass="math inline">\(\alpha\)</span>项已知，于是近似的方差就可以是<span style="color: #eb52f7"> <spanclass="math"><spanclass="math inline">\(\Sigma_q(t)=\sigma_q^2(t)\mathbf I\)</span></span></span>.因此必须参数化它的均值<span style="color: #4eb31c"> <spanclass="math"><spanclass="math inline">\(\mu_\theta(x_t,t)\)</span></span> </span>.</p><p>两个高斯分布之间的KL散度：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204601419.png" /></p><p><span class="math display">\[D_{KL}(\mathcal N(x;\mu_x,\Sigma_x)\parallel\mathcalN(y;\mu_y,\Sigma_y))=\frac{1}{2}\bigg[\log\frac{|\Sigma_y|}{|\Sigma_x|}-d+tr(\Sigma_y^{-1}\Sigma_x)+(\mu_y-\mu_x)^T\Sigma_y^{-1}(\mu_y-\mu_x)\bigg]\tag{86}\]</span></p><p>那么<strong>去噪匹配项</strong>中的KL散度可以通过最小化这两个分布的<spanstyle="background-color: #ff666680">均值</span>差异来减小：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204609882.png" /></p><p><span class="math display">\[\mu_q(x_t,x_0)=\frac{\sqrt\alpha_t(1-\bar\alpha_{t-1})x_t+\sqrt{\bar\alpha_{t-1}}(1-\alpha_t)x_0}{1-\bar\alpha_t}\tag{93}\]</span></p><p><span class="math display">\[\mu_\theta(x_t,t)=\frac{\sqrt\alpha_t(1-\bar\alpha_{t-1})x_t+\sqrt{\bar\alpha_{t-1}}(1-\alpha_t)\hatx_\theta(x_t,t)}{1-\bar\alpha_t}\tag{94}\]</span></p><p>因此优化问题简化为：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204624272.png" /></p><p>因此优化VDM归结起来就是学习一个NN从任意噪声版本的图像中预测出原始图像。此外，最小化所有噪声水平上公式<spanclass="math inline">\((58)\)</span>中的求和项可以通过最小化所有时间步的期望来近似：</p><p><span class="math display">\[\underset{\theta}{arg\ min}\ \mathbb E_{t\sim U{2,T}}\big[\mathbbE_{q(x_t|x_0)}[D_{KL}(q(x_{t-1}|x_t,x_0)\parallelp_\theta(x_{t-1}|x_t))]\big]\tag{100}\]</span></p><p>可以在每个时间步使用随机采样来优化。</p><h2 id="学习噪声参数">学习噪声参数</h2><p>如何学习噪声参数？一种方法是使用一个NN <spanclass="math inline">\(\hat\alpha_\eta(t)\)</span> 来建模 <spanclass="math inline">\(\alpha_t\)</span> ，但是很低效。解决方案：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204631290.png" /></p><p>由于 <span class="math inline">\(q(x_t|x_0)\sim \mathcalN(x_t;\sqrt{\bar\alpha_t}x_0,(1-\bar\alpha_t)\mathbf I)\)</span>,根据信噪比定义 <spanclass="math inline">\(\mathrm{SNR}=\frac{\mu^2}{\sigma^2}\)</span>,，每个时间步的SNR为：</p><p><span class="math display">\[\mathrm{SNR}(t)=\frac{\bar\alpha_t}{1-\bar\alpha_t}\tag{109}\]</span></p><p><span class="math display">\[\frac{1}{2\sigma_q^2(t)}\frac{\bar\alpha_{t-1}(1-\alpha_t)^2}{(1-\bar\alpha_t)^2}\Big[\left\|\hatx_\theta(x_t,t)-x_0\right\|_2^2\Big]=\frac{1}{2}(\mathrm{SNR}(t-1)-\mathrm{SNR}(t))\Big[\left\|\hatx_\theta(x_t,t)-x_0\right\|_2^2\Big]\tag{110}\]</span></p><p>SNR表示原始信号与噪声之间的比值，在扩散中我们需要SNR随着t的增加而减小，于是可以表示为：</p><p><span class="math display">\[\mathrm{SNR}(t)=exp(-\omega_\eta(t))\tag{111}\]</span></p><p><span class="math inline">\(\omega_\eta(t)\)</span>是一个单调递增的NN，于是结合以上的式子就可以优雅的表示 <spanclass="math inline">\(\bar\alpha_t\)</span> 以及 <spanclass="math inline">\(1-\bar\alpha_t\)</span> 的值：</p><p><span class="math display">\[\frac{\bar\alpha_t}{1-\bar\alpha_t}=\exp(-\omega_\eta(t))\tag{112}\]</span></p><p><span class="math display">\[\therefore \bar\alpha_t=sigmoid(-\omega_\eta(t))\tag{113}\]</span></p><p><span class="math display">\[\therefore 1-\bar\alpha_t=sigmoid(\omega_\eta(t))\tag{114}\]</span></p><h2 id="三种等价形式">三种等价形式</h2><h3 id="预测噪声">预测噪声</h3><p>根据公式 <span class="math inline">\((69)\)</span> ：</p><p><span class="math display">\[x_0=\frac{x_t-\sqrt{1-\bar\alpha_t}\epsilon_0}{\sqrt{\bar\alpha_t}}\tag{115}\]</span></p><p>代入到均值<span style="color: #eb52f7"> <span class="math"><spanclass="math inline">\(\mu_q(x_t,x_0)\)</span></span> </span>：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204640841.png" /></p><p>于是近似的均值<span style="color: #4eb31c"> <span class="math"><spanclass="math inline">\(\mu_\theta(x_t,t)\)</span></span> </span>：</p><p><span class="math display">\[\mu_\theta(x_t,t)=\frac{1}{\sqrt\alpha_t}x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}\sqrt\alpha_t}\hat\epsilon_\theta(x_t,t)\tag{125}\]</span></p><p>那么优化项就变为：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204649500.png" /></p><p>因此，通过预测原始图像 <span class="math inline">\(x_0\)</span>学习VDM等价于去学习预测噪声，另外，一些工作也发现预测噪声会有更好的表现。</p><h3 id="预测分数">预测分数</h3><p><strong><spanstyle="color: #ff2020">Tweedie公式</span></strong>：<u>给定从指数族分布中的样本的真实平均值可以通过样本的最大似然估计（经验平均值）加上一些涉及估计分数的校正项得到</u></p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204656451.png" /></p><p><span class="math display">\[\mathbb E[\mu_z|z]=z+\Sigma_z\nabla_z\log p(z)\]</span></p><p>在VDM里面，通常被用来减少样本的偏差，如果观测样本都在潜在分布的一端，那么负的分数将会变大并且校正原始的样本的最大似然估计为正确的均值。我们使用它来预测给定后验器样本后的<span class="math inline">\(x_t\)</span> 的真实均值，我们知道：<spanclass="math inline">\(q(x_t|x_0)=\mathcalN(x_t;\sqrt{\bar\alpha_t}x_0,(1-\bar\alpha_t)\mathbf I)\)</span>，于是根据 Tweedie公式：</p><p><span class="math display">\[\mathbb E[\mu_{x_t}|x_t]=x_t+(1-\bar\alpha_t)\nabla_{x_t}\logp(x_t)\tag{131}\]</span></p><p>于是：</p><p><span class="math display">\[\sqrt{\bar\alpha_t}x_0=x_t+(1-\bar\alpha_t)\nabla\log p(x_t)\tag{132}\]</span></p><p><span class="math display">\[\therefore x_0=\frac{x_t+(1-\bar\alpha_t)\nabla\logp(x_t)}{\sqrt{\bar\alpha_t}}\tag{133}\]</span></p><p>再次代入到均值<span style="color: #eb52f7"> <span class="math"><spanclass="math inline">\(\mu_q(x_t,x_0)\)</span></span> </span>：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204703788.png" /></p><p>于是近似的均值<span style="color: #4eb31c"> <span class="math"><spanclass="math inline">\(\mu_\theta(x_t,t)\)</span></span> </span>：</p><p><span class="math display">\[\mu_\theta(x_t,t)=\frac{1}{\sqrt\alpha_t}x_t+\frac{1-\alpha_t}{\sqrt\alpha_t}s_\theta(x_t,t)\tag{143}\]</span></p><p>那么优化项就变为：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204716491.png" /></p><p>在这里，<span class="math inline">\(s_\theta(x_t,t)\)</span>是一个NN用来学习预测分数函数 <spanclass="math inline">\(\nabla_{x_t}\log p(x_t)\)</span>,也就是在任意噪声水平 <span class="math inline">\(t\)</span> 下， <spanclass="math inline">\(x_t\)</span> 在数据空间中的梯度</p><h3 id="二者联系">二者联系</h3><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204723658.png" /></p><p>可以看到，两者之间存在一个随时间变化的常数因子！分数函数衡量应该在数据空间中如何移动来最大化对数似然；</p><blockquote><p><span class="highlight"data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F12736052%2Fitems%2FWHZV587X%22%2C%22annotationKey%22%3A%22X3LSTVZJ%22%2C%22color%22%3A%22%23ff6666%22%2C%22pageLabel%22%3A%2217%22%2C%22position%22%3A%7B%22pageIndex%22%3A16%2C%22rects%22%3A%5B%5B341.366%2C297.306%2C539.996%2C307.269%5D%2C%5B72%2C285.351%2C540%2C295.314%5D%2C%5B72%2C273.395%2C290.539%2C283.358%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F12736052%2Fitems%2FJC7NRMH3%22%5D%2C%22locator%22%3A%2217%22%7D%7D"data-ztype="zhighlight"><a href="zotero://open-pdf/library/items/WHZV587X?page=17&#x26;annotation=X3LSTVZJ">“intuitively,since the source noise is added to a natural image to corrupt it, movingin its opposite direction "denoises" the image and would be the bestupdate to increase the subsequent log probability.”</a></span> <spanclass="citation"data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F12736052%2Fitems%2FJC7NRMH3%22%5D%2C%22locator%22%3A%2217%22%7D%5D%2C%22properties%22%3A%7B%7D%7D"data-ztype="zcitation">(<spanclass="citation-item"><a href="zotero://select/library/items/JC7NRMH3">Luo,2022, p. 17</a></span>)</span>🔤直观上，由于源噪声被添加到自然图像中以破坏它，因此沿其相反方向移动会对图像进行“去噪”，并且将是增加后续对数概率的最佳更新。🔤</p></blockquote><p>于是，学习建模分数函数就等价于建模原噪声的负值(乘上一个比例因子)</p><p>因此，我们有三种等价目标：预测原始图像 <spanclass="math inline">\(x_0\)</span> 、源噪声 <spanclass="math inline">\(\epsilon_0\)</span> 、任意噪声水平时的图像的分数<span class="math inline">\(\nabla\log p(x_0)\)</span></p>]]></content>
    
    
    
    <tags>
      
      <tag>Diffusion</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IR-SDE</title>
    <link href="/2023/11/13/IR-SDE/"/>
    <url>/2023/11/13/IR-SDE/</url>
    
    <content type="html"><![CDATA[<h1id="image-restoration-with-mean-reverting-stochastic-differential-equations">ImageRestoration with Mean-Reverting Stochastic Differential Equations</h1><h2 id="背景">背景</h2><p>以往的方法的反向过程会初始化一个较高方差的噪声，可能会导致ground-truth高质量图像的恢复效果不好，虽然可以获得较高的感知分数，但是往往会在基于像素或结构的退化标准方面表现不太满意。</p><h2 id="贡献">贡献</h2><p>为一般的IR问题提出一种SDE方法。mean-recertingSDE关键思想：<strong>将高质量的图像转换为退化图像，作为具有固定高斯噪声的均值状态</strong>，然后去估计reverse-timeSDE，不需要有任何task-specific的先验知识。提出了最大似然目标学习优化的reverse轨迹。</p><h2 id="方法">方法</h2><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204203262.png" /></p><h3 id="前向过程">前向过程</h3><p><span class="math display">\[dx=\theta_t(\mu-x)dt+\sigma_tdw\]</span></p><p><span class="math inline">\(\mu\)</span> 为状态均值，<spanclass="math inline">\(\theta_t,\sigma_t\)</span>是与时间相关的正参数，分别表征了均值恢复和随机波动的速度。<spanclass="math inline">\(\mu\)</span> 与 <spanclass="math inline">\(x(0)\)</span>分别为退化的LQ图像和ground-truth的HQ图像对。由于<spanclass="math inline">\(\mu\)</span> 依赖于<spanclass="math inline">\(x(0)\)</span> ，<spanclass="math inline">\(x(0)\)</span>与布朗运动无关，因此该SDE在伊藤形式上依然有效。 令<spanclass="math inline">\(\sigma_t^2/\theta_t=2\lambda^2\)</span> ，<spanclass="math inline">\(\lambda^2\)</span> 为固定的方差。给定任意的 <spanclass="math inline">\(x(s),s&lt;t\)</span> ，SDE的解为: <spanclass="math display">\[x(t)=\mu+(x(s)-\mu)e^{-\bar\theta_{s:t}}+\int_s^t\sigma_ze^{-\bar\theta_{z:t}}dw(z)\]</span> <spanclass="math inline">\(\bar\theta_{s:t}:=\int_s^t\theta_zdz\)</span>已知，则中间的转移步 <span class="math inline">\(p(x(t)|x(s))=\mathcalN(x(t)|m_{s:t}(x(s)),v_{s:t})\)</span> ： <span class="math display">\[m_{s:t}(x(s)):=\mu+(x(s)-\mu)e^{-\bar\theta_{s:t}},\\v_{s:t}:=\int_s^t\sigma_z^2e^{-2\bar\theta_{z:t}}dz\\=\lambda^2(1-e^{-2\bar\theta_{s:t}})\]</span></p><blockquote><p>证明：</p><p>需求解的SDE： <span class="math display">\[\mathrm dx=\theta_t(\mu-x)\mathrm dt+\sigma_t \mathrm dw\]</span> 为简便，使用 <span class="math inline">\(\bar\theta_t\)</span>替换 <span class="math inline">\(\bar\theta_{0:t}\)</span>，定义一个代理可微函数(方便求解)： <span class="math display">\[\psi(x,t)=x\mathrm e^{\bar\theta_t}\]</span> 根据伊藤公式： <span class="math display">\[\mathrm d\psi(x,t)=\frac{\partial\psi}{\partial t}(x,t)\mathrmdt+\frac{\partial\psi}{\partial x}(x,t)\mathbf f(x,t)\mathrm dt\\+\frac12\frac{\partial^2\psi}{\partial x^2}(x,t)g(t)^2\mathrm dt\\+\frac{\partial\psi}{\partial x}(x,t)g(t)\mathrm {dw_t}\]</span></p><p>将式 <span class="math inline">\((4)\)</span> 代入 <spanclass="math inline">\((6)\)</span> 得: <span class="math display">\[\mathrm d\psi(x,t)=\mu\theta_t\mathrm e^{\bar\theta_t}\mathrmdt+\sigma_t\mathrm e^{\bar\theta_t}\mathrm{dw}_t\]</span> 积分得： <span class="math display">\[\psi(x,t)-\psi(x,s)=\int_s^t\mu\theta_z\mathrm e^{\bar\theta_z}\mathrmdz+\int_s^t\sigma_z\mathrm e^{\bar\theta_z}\mathrm{dw}(z)\]</span> 在这里，<span class="math inline">\(\int_s^t\sigma_z\mathrme^{\bar\theta_z}\mathrm{dw}(z)\sim\mathcal N(0,\int_s^t\sigma^2_z\mathrme^{2\bar\theta_z}dz)\)</span></p><blockquote><p>证明： <span class="math display">\[Var(\int_s^tH_zdW(z))\approxVar(\sum_{i=1}^NH_{t_i}(W_{t_i}-W_{t_{i-1}}))\\=\sum_{i=1}^NVar(H_{t_i}(W_{t_i}-W_{t_{i-1}}))\\=\sum_{i=1}^NH_{t_i}^2Var(W_{t_i}-W_{t_{i-1}})\\=\sum_{i=1}^NH_{t_i}^2(t_i-t_{i-1})\]</span></p><p><span class="math display">\[\lim_{max \Deltat_i\to0}\sum_{i=1}^NH_{t_i}^2(t_i-t_{i-1})=\int_s^tH_z^2dz\]</span></p></blockquote><p>由于 <span class="math inline">\(\mathrm d\bar\theta_t=\mathrmd\int_0^t\theta_z\mathrm dz=\theta_t\)</span> ,于是： <spanclass="math display">\[x(t)\mathrm e^{\bar\theta_t}-x(s)\mathrm e^{\bar\theta_s}=\mu(\mathrme^{\bar\theta_t}-\mathrm e^{\bar\theta_s})+\int_s^t\sigma_z\mathrme^{\bar\theta_z}\mathrm{dw}(z)\]</span> 两边同时除以 <span class="math inline">\(\mathrme^{\bar\theta_t}\)</span> 得： <span class="math display">\[x(t)=\mu+(x(s)-\mu)\mathrm e^{-\bar\theta_{s:t}}+\int_s^t\sigma_z\mathrme^{-\bar\theta_{z:t}}\mathrm {dw}_z\]</span></p><p><span class="math display">\[\int_s^t\sigma_z^2\mathrm e^{-2\bar\theta_{z:t}}\mathrmdz=\frac{\sigma_t^2}{2\theta_t}e^0-\frac{\sigma^2_s}{2\theta_s}\mathrme^{-2\bar\theta_{s:t}}=\lambda^2(1-\mathrm e^{-2\bar\theta_{s:t}})\]</span></p><p>证毕</p></blockquote><p>当 <span class="math inline">\(t\to\infty\)</span> 时，<spanclass="math inline">\(m_t\)</span> 趋于LQ图像 <spanclass="math inline">\(\mu\)</span> ，<spanclass="math inline">\(v_t\)</span> 趋于 <spanclass="math inline">\(\lambda^2\)</span>，即SDE将HQ图像扩散到LQ图像加一张固定高斯噪声。</p><h3 id="反向过程">反向过程</h3><p><span class="math display">\[\mathrm dx=[\theta_t(\mu-x)-\sigma^2_t\nabla_x\logp_t(x)]dt+\sigma_t\mathrm d\hat w\]</span></p><p>每一步的ground-truth score: <span class="math display">\[\nabla_x\log p_t(x|x(0))=-\frac{x(t)-m_t(x)}{v_t}\]</span> 重新参数化 <spanclass="math inline">\(x(t)=m_t(x)+\sqrt{v_t}\epsilon_t,\epsilon\sim\mathcaln(0,I)\)</span> ,于是： <span class="math display">\[\nabla_x\log p_t(x|x(0))=-\frac{\epsilon_t}{\sqrt v_t}\]</span> 训练噪声估计网络 <spanclass="math inline">\(\tilde\epsilon_\phi(x(t),\mu,t)\)</span> : <spanclass="math display">\[L_\gamma(\phi):=\sum_{i=1}^{T}\gamma_i\mathbbE[\left\|\tilde\epsilon_\phi(x(t),\mu,t)-\epsilon_i\right\|]\]</span>上式虽然比较简单，但是往往应用到复杂退化场景时会不稳定。原因：试图学习给定时间的瞬时噪声。</p><p><strong>解决方法：</strong> 提出最大似然目标尝试寻找给定 <spanclass="math inline">\(x_0\)</span> 后 <spanclass="math inline">\(x_{1:T}\)</span>的最优轨迹（而不是更准确的分数函数）</p><p>最大似然目标： <span class="math display">\[p(x_{1:T}|x_0)=p(x_T|x_0)\prod_{i=2}^Tp(x_{i-1}|x_i,x_0)\]</span> 于是最佳反向状态为最小化负的对数似然： <spanclass="math display">\[x_{i-1}^*=\underset{x_{i-1}}{\mathrm{arg\ min}}[-\logp(x_{i-1}|x_i,x_0)]\]</span> 令 <spanclass="math inline">\(\theta&#39;_i:=\int_{i-1}^i\theta_t\mathrmdt\)</span> ,通过求解可得最优轨迹： <span class="math display">\[x_{i-1}^*=\frac{1-\mathrm e^{-2\bar\theta_{i-1}}}{1-\mathrme^{-2\bar\theta_i}}\mathrm e^{-\theta&#39;_i}(x_i-\mu)\\+\frac{1-\mathrm e^{-2\theta_i&#39;}}{1-\mathrme^{\bar\theta_i}}(x_0-\mu)+\mu\]</span></p><blockquote><p>证明：</p><p>根据贝叶斯规则 <span class="math display">\[-\logp(x_{i-1}|x_i,x_0)=-\log\frac{p(x_i|x_{i-1},x_0)p(x_{i-1}|x_0)}{p(x_i|x_0)}\\\propto -\log p(x_i|x_{i-1},x_0)-\log p(x_{i-1}|x_0)\]</span> 令负的对数似然的梯度为0：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204213481.png" /></p><p>证毕，二阶导数为正，因此 <spanclass="math inline">\(x_{i-1}^*\)</span> 确实为最优点</p></blockquote><p>训练噪声估计网络 <spanclass="math inline">\(\tilde\epsilon_\phi(x_i,\mu,i)\)</span> : <spanclass="math display">\[J_\gamma(\phi):=\sum_{i=1}^T\gamma_i\mathbbE[\left\|x_i-(dx_i)_{\tilde\epsilon_\phi}-x_{i-1}^*\right\|]\]</span> <spanclass="math inline">\((dx_i)_{\tilde\epsilon_\phi}\)</span> 为式 <spanclass="math inline">\((14)\)</span> 的反向SDE，其中分数由 <spanclass="math inline">\(\tilde\epsilon_\phi\)</span>估计，由于扩散项的期望为0，于是只需要考虑漂移项。</p><h2 id="实验">实验</h2><h3 id="deraining">Deraining</h3><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204222675.png" /></p><h3 id="debluring">Debluring</h3><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204230422.png" /></p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204238169.png" /></p><h3 id="gaussian-image-denoising">Gaussian Image Denoising</h3><p>提出Denoising-SDE：将干净图像设为均值 <spanclass="math inline">\(\mu=x_0\)</span>，可以以更少的时间步进行去噪。因此可以将任意噪声图作为中间状态，将其直接反转为干净图像。</p><p>提出Denoising-ODE：由于只有高斯噪声，因此可以直接使用ODE，没有额外的噪声直接去噪：<span class="math display">\[\mathrm dx=[\theta_t(\mu-x)-\sigma^2_t\nabla_x\log p_t(x)]dt\]</span></p><blockquote><p><span class="math display">\[p_{noise}(x_i|x_0)=\mathcal N(m_i(x_0),v_i)\]</span></p><p>这里的 <spanclass="math inline">\(m_i(x_0)=x_0,v_i=\lambda^2(1-e^{-2\bar\theta_i})\)</span>，最优轨迹变为： <span class="math display">\[x_{i-1}^*=\frac{1-\mathrm e^{-2\bar\theta_{i-1}}}{1-\mathrme^{-2\bar\theta_i}}\mathrm e^{-\theta&#39;_i}(x_i-x_0)+x_0\]</span></p><p><span class="math display">\[x_i=m_i+\sqrt{v_i}\epsilon_t,\epsilon\sim\mathcal N(0,I)\]</span></p><p>将式 <span class="math inline">\((26)\)</span> 代入式 <spanclass="math inline">\((14)\)</span> 得Denoising-SDE:</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204248254.png" /></p><p>对应的ODE： <span class="math display">\[dx=-\frac12\sigma_t^2\mathrm e^{-2\bar\theta_t}\nabla_{x_t}\log p_t(x)dt\]</span> 一旦知道真实噪声水平 <spanclass="math inline">\(\sigma_{real}\)</span> 则可以直接推出相应的 <spanclass="math inline">\(t^*\)</span> 使得 <spanclass="math inline">\(p_{noise}(x_i|x_0)\)</span> 的方差刚好为噪声水平：<span class="math display">\[\sigma_{real}^2=v_t=\lambda^2(1-\mathrm e^{-2\bar\theta_t})\]</span> 于是： <span class="math display">\[t^*=\underset{t}{arg\ min}\left\|\bar\theta_t-\frac1{2\Delta t}\log(1-\frac{\sigma_{real}^2}{\lambda^2    })\right\|\]</span></p></blockquote><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204257143.png" /></p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204304756.png" /></p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207204311705.png" /></p><h2 id="局限与未来方向">局限与未来方向</h2><p><span class="math inline">\(v_t\)</span>中的指数项会使得在最后几步的方差变得过于平滑，使得相邻状态很相似，使得学习变得困难。<strong>解决方案</strong>：改进<span class="math inline">\(\theta\)</span> 时间表。</p><p>采样步骤的优化，以减少计算成本</p><h2 id="参考文献">参考文献</h2><p>Luo, Ziwei, Fredrik K. Gustafsson, Zheng Zhao, Jens Sjölund, andThomas B. Schön. “Image Restoration with Mean-Reverting StochasticDifferential Equations.” arXiv, May 31, 2023.http://arxiv.org/abs/2301.11699.</p>]]></content>
    
    
    
    <tags>
      
      <tag>SDE</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于分数的生成模型</title>
    <link href="/2023/11/12/Understanding%20diffusion%E5%9F%BA%E4%BA%8E%E5%88%86%E6%95%B0%E7%9A%84%E6%A8%A1%E5%9E%8B/"/>
    <url>/2023/11/12/Understanding%20diffusion%E5%9F%BA%E4%BA%8E%E5%88%86%E6%95%B0%E7%9A%84%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="基于分数的生成模型">基于分数的生成模型</h1><h2 id="基于能量的模型">基于能量的模型</h2><p>任意分布都可以被写为： <span class="math display">\[p_\theta(x)=\frac{1}{Z_\theta}e^{-f_\theta(x)}\tag{152}\]</span></p><p><span class="math inline">\(f_\theta(x)\)</span>是一个任意的、参数化的函数——能量函数，通常被建模为一个NN，<spanclass="math inline">\(Z_\theta\)</span> 是正则化常数使得 <spanclass="math inline">\(\int p_\theta(x)dx=1\)</span>,如果使用最大似然估计，对复杂的 <spanclass="math inline">\(f_\theta(x)\)</span> 来说，<spanclass="math inline">\(Z_\theta\)</span> 可能难以处理</p><h2 id="分数模型">分数模型</h2><p>上面的问题可以使用一个NN <spanclass="math inline">\(s_\theta(x)\)</span> 来学习 <spanclass="math inline">\(p(x)\)</span> 的分数函数 <spanclass="math inline">\(\nabla\log p(x)\)</span> 所避免。动机：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207200915631.png" /></p><p>因此不需要表示任何正则化常数，分数模型可以通过最小化Fisher散度来优化：</p><p><span class="math display">\[\mathbb E_{p(x)}\Big[\left\|s_\theta(x)-\nabla\logp(x)\right\|_2^2\Big]\tag{157}\]</span> 对于每个 <span class="math inline">\(x\)</span>来说，它的对数似然的梯度根本上描述了在数据空间中进一步增加它的对数似然的方向。直觉上，分数函数在数据x 所在的整个空间上定义了一个向量场，指向众数：</p><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207200928758.png" /></p><p>通过学习真实数据分布的分数函数，我们可以从相同空间中的任一点开始，逐渐地跟随分数直到模式到达来生成样本。采样过程为已知的郎之万采样：<span class="math display">\[x_{i+1}\gets x_i+c\nabla\log p(x_i)+\sqrt{2c}\epsilon,\i=0,1,...,K\tag{158}\]</span> <span class="math inline">\(x_0\)</span>是从先验分布中随机采样，<span class="math inline">\(\epsilon\sim\mathcalN(\epsilon;\mathbf 0,\mathbf I)\)</span>是一个额外的噪声项用来保证生成样本不总是崩溃到一个模式，而是为了多样性环绕在它周围，保留一定的随机性。由于ground-truth分数函数对于复杂分布的自然图像来说不可获取，于是使用分数匹配方法来解决。</p><p>普通分数匹配存在三个问题：</p><ol type="1"><li><p>当 <span class="math inline">\(x\)</span>在高维空间的低维流形上时，分数函数是不明确的。所有不在低维流形的点的概率为0，此时对数没有定义，而自然图像通常在整个环境空间的低维流形上</p></li><li><p>通过普通分数匹配训练的分数函数在低密度区域可能是不正确的，因为我们最小化的目标是关于<span class="math inline">\(p(x)\)</span>的期望，从它的样本中训练，模型无法接收到很少见或者看不到的样本的正确的学习信号。（采样策略是高维空间的随机位置开始，很可能是随机噪声）</p></li><li><p>郎之万动力采样可能不会拟合，即使使用ground-truth分数：</p><blockquote><p>假设真实数据分布是两个不相交的分布的混合 <spanclass="math display">\[p(x)=c_1p_1(x)+c_2p_2(x)\tag{159}\]</span>在计算分数时，它们的混合系数被丢掉，那么从它们中间的初始点使用郎之万动力学采样的话，到达它们每个模式的机会都是粗略相等的，尽管可能某一个分布的权重更大</p></blockquote><p><imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207200936664.png" /></p><p><strong>解决方案</strong>：在数据中加入多个水平的高斯噪声(去噪分数匹配)</p><ol type="1"><li>由于高斯噪声的假设是整个空间，于是扰动后的数据不再局限于低维流形</li><li>加入较大的高斯噪声将增大每种模式在数据分布中覆盖的面积，在低密度区域增加了更多训练信号</li><li>加入多个水平的高斯噪声并增加方差将使得结果的中间分布尊重ground-truth的混合系数</li></ol><p>于是，形式上，选择正的噪声水平序列 <spanclass="math inline">\(\{\sigma_t\}_{t=1}^T\)</span>，以及定义一个逐渐扰动数据分布的序列： <span class="math display">\[p_{\sigma_t}(x_t)=\int p(x)\mathcal N(x_t;x,\sigma_t^2\mathbfI)dx\tag{160}\]</span> <span class="math inline">\(s_\theta(x,t)\)</span>使用<strong>分数匹配</strong>训练学习同时所有噪声水平的分数函数： <spanclass="math display">\[\underset{\theta}{arg\ min}\sum_{t=1}^T\lambda(t)\mathbbE_{p_{\sigma_t}(x_t)}\Big[\left\|s_\theta(x,t)-\nabla\logp_{\sigma_t}(x_t)\right\|_2^2\Big]\tag{161}\]</span></p></li></ol><p><span class="math inline">\(\lambda(t)\)</span>是正的权重函数，通常被选择为 <spanclass="math inline">\(\lambda(i)=\sigma_i^2\)</span>由于噪声水平随着时间稳定下降，因此要随着时间减少步长，使得样本最后集中在真实的模式上。<span class="math display">\[\nabla_{x_t}\log p_{\sigma_t}(x_t)=\nabla_{x_t}\logp_{\sigma_t}(x_t|x)p(x)=\nabla_{x_t}\log p_{\sigma_t}(x_t|x)\tag{162}\]</span></p><p><span class="math display">\[\nabla_{x_t}\log p_{\sigma_t}(x_t|x)=\nabla_{x_t}\log\Big\{\frac{1}{\sqrt{2\pi}\sigma_t}\exp\Big(-\frac{(x_t-x)^2}{2\sigma_t^2}\Big)\Big\}\\=\nabla_{x_t}\Big[-\frac{(x_t-x)^2}{2\sigma_t^2}\Big]\\=-\frac{x_t-x}{\sigma_t^2}\\=-\frac{\epsilon}{\sigma_t}\tag{163}\]</span> 于是优化目标可以写为： <span class="math display">\[\underset{\theta}{arg\ min}\sum_{t=1}^{T}\lambda(t)\mathbbE_{p_{\sigma_t}(x_t)}\Bigg[\left\|s_\theta(x,t)+\frac{x_t-x}{\sigma_t^2}\right\|_2^2\Bigg]\\=\underset{\theta}{arg\ min}\sum_{t=1}^{T}\mathbbE_{p_{\sigma_t}(x_t)}\Big[\left\|\sigma_ts_\theta(x,t)+\epsilon\right\|_2^2\Big]\tag{164}\]</span></p><h2 id="sde推导">SDE推导</h2><p>众所周知，扩散模型中 <span class="math inline">\(x_0\to x_T,x_T\tox_0\)</span> 的过程一种随机过程，于是考虑使用随机微分方程（StochasticDifferential Equations，SDE）来刻画。</p><p>原始扩散过程为离散形式，使用SDE进行连续的描述，于是前向扩散过程可以看作一个伊藤扩散过程的解：<span class="math display">\[dx=f(x,t)dx+g(t)d\omega\]</span> 其中： <span class="math display">\[dx=\lim\limits_{\Delta t\to0}(x_{t+\Delta t}-x_t)\]</span> 因此原来离散形式的 <span class="math inline">\(x_t\tox_{t+1}\)</span> 就变为连续形式的 <span class="math inline">\(x_{t}\tox_{t+\Delta t}\)</span> ，于是： <span class="math display">\[x_{t+\Delta t}=x_t+\underset{确定部分}{f(x,t)\Deltat}+\underset{随机部分}{g(t)\sqrt{\Delta t}\epsilon},\epsilon\sim\mathcalN(0,I)\]</span> 原来离散的前向过程：<span class="math inline">\(x_0\to x_1\tox_2\to ...\to x_T\)</span> ，现在连续的前向过程：<spanclass="math inline">\(x_0\to x_{\Delta t}\to x_{2\Delta t}\to ... \tox_T\)</span></p><p>有了前向过程的SDE后，需要分析反向过程以还原真实分布， <spanclass="math display">\[p(x_{t+\Delta t}|x_t)=\mathcal N(x_{t+\Delta t};x_t+f(x,t)\Deltat,g^2(t)\Delta tI)\\\propto\exp(\frac{\left\|x_{t+\Delta t}-x_t-f(x,t)\Deltat\right\|^2}{2g^2(t)\Delta t})\]</span> 根据贝叶斯规则： <span class="math display">\[p(x_t|x_{t+\Delta t})=\frac{p(x_{t+\Delta t}|x_t)p(x_t)}{p(x_{t+\Deltat})}\\=p(x_{t+\Delta t}|x_t)\exp(\log p(x_t)-\log p(x_{t+\Delta t}))\\=\exp(-\frac{\left\|x_{t+\Delta t}-x_t-f(x,t)\Deltat\right\|^2}{2g^2(t)\Delta t}+\log p(x_t)-\log p(x_{t+\Deltat}))\label{6}\]</span> 对 <span class="math inline">\(\log p(x_{t+\Delta t})\)</span>进行泰勒展开： <span class="math display">\[\log p(x_{t+\Delta t})\approx \log p(x_t)+(x_{t+\Deltat}-x_t)\nabla_{x_t}\log p(x_t)+\Delta t\frac{\partial \logp(x_t)}{\partial t}\label{7}\]</span> 将公式<spanclass="math inline">\(\eqref7\)</span>代入公式<spanclass="math inline">\(\eqref6\)</span>得： <span class="math display">\[p(x_t|x_{t+\Delta t})\propto \exp(-\frac{\left\|x_{t+\Deltat}-x_t-f(x,t)\Delta t\right\|^2}{2g^2(t)\Delta t}+(x_{t+\Deltat}-x_t)\nabla_{x_t}\log p(x_t)-\Delta t\frac{\partial \logp(x_t)}{\partial t})\\=\exp(-\frac{\left\|x_{t+\Delta t}-x_t-[f(x,t)-g^2(t)\nabla_{x_t}\logp(x_t)]\Delta t\right\|^2}{2g^2(t)\Delta t}+\mathscr Ot)\]</span> 当 <span class="math inline">\(\Delta t\to0\)</span> 时，<spanclass="math inline">\(\mathscr Ot\to0\)</span> ，于是： <spanclass="math display">\[p(x_t|x_{t+\Delta t})\propto \exp(-\frac{\left\|x_{t+\Deltat}-x_t-[f(x,t)-g^2(t)\nabla_{x_t}\log p(x_t)]\Deltat\right\|^2}{2g^2(t)\Delta t})\\\approx \exp(-\frac{\left\|x_t-x_{t+\Delta t}+[f(x,t+\Deltat)-g^2(t+\Delta t)\nabla_{x_{t+\Delta t}}\log p(x_{t+\Delta t})]\Deltat\right\|^2}{2g^2(t+\Delta t)\Delta t})\]</span> 因此： <span class="math display">\[p(x_t|x_{t+\Delta t})=\mathcal N(x_t;x_{t+\Delta t}-[f(x,t+\Deltat)-g^2(t+\Delta t)\nabla_{x_{t+\Delta t}}\log p(x_{t+\Delta t})]\Deltat,g^2(t+\Delta t)\Delta tI)\]</span> 取 <span class="math inline">\(\Delta t\to0\)</span> ，以及<span class="math inline">\(dx=\lim\limits_{\Delta t\to0}(x_{t+\Deltat}-x_t)\)</span> 得最后的==reverse-time SDE==： <spanclass="math display">\[dx=[f(x,t)-g^2(t)\nabla_x\log p_t(x)]dt+g(t)d\omega\]</span> <imgsrc="https://typoracc.oss-cn-beijing.aliyuncs.com/img/image-20231207200945593.png" /></p><h3 id="ve-sde-vs.-vp-sde">VE-SDE vs. VP-SDE</h3><p>将NCSN以及DDPM均考虑到SDE框架中，分别为Variance Exploding(VE)SDE和Variance Preserving(VP) SDE。</p><h4 id="ve-sde">VE-SDE</h4><p>对于NCSN，扩散公式为： <span class="math display">\[x_T=x_0+\sigma_T\varepsilon\]</span> 依靠非常大的 <span class="math inline">\(\sigma_T\)</span>使得 <span class="math inline">\(x_T\)</span> 变为以 <spanclass="math inline">\(\epsilon\)</span> 主导的高斯噪声，因此是VarianceExploding（方差爆炸）。</p><p>NCSN每一步的扰动核 <spanclass="math inline">\(p_{\sigma_i}(x|x_0)\)</span> 的马尔科夫链表示：<span class="math display">\[x_i=x_{i-1}+\sqrt{\sigma_i^2-\sigma_{i-1}^2}z_{i-1},i=1,...,N\]</span> 这里的 <span class="math inline">\(z_{i-1}\sim\mathcalN(0,I)\)</span> ,当 <span class="math inline">\(N\to\infty\)</span>时，马尔科夫链 <span class="math inline">\(\{x_i\}_{i=1}^N\)</span>就变成了连续的随机过程 <spanclass="math inline">\(\{x(t)\}_{t=0}^1\)</span> ,<spanclass="math inline">\(\{\sigma_i\}_{i=1}^N\)</span> 变为函数 <spanclass="math inline">\(\sigma(t)\)</span> , <spanclass="math inline">\(z_i\)</span> 变为 <spanclass="math inline">\(z(t)\)</span> , 这里的连续时间变量 <spanclass="math inline">\(t\in [0,1]\)</span> , 而不是整数 <spanclass="math inline">\(i\in \{1,2,...,N\}\)</span>. 令 <spanclass="math inline">\(x(\frac{i}{N})=x_i,\sigma(\frac{i}{N})=\sigma_i,z(\frac{i}{N})=z_i\)</span>, 于是 <span class="math inline">\(\Deltat=\frac1N,t\in\{0,\frac1N,...,\frac{N-1}{N}\}\)</span> : <spanclass="math display">\[x(t+\Delta t)=x(t)+\sqrt{\sigma^2(t+\Delta t)-\sigma^2(t)}z(t)\\=x(t+\sqrt{\frac{\sigma^2(t+\Delta t)^2-\sigma^2(t)}{\Deltat}}\sqrt{\Delta t}z(t)\\=x(t)+\sqrt{\frac{\Delta\sigma^2}{\Delta t}}\sqrt{\Delta t}z(t)\]</span> 于是当 <span class="math inline">\(\Delta t\to0\)</span> 时：<span class="math display">\[f(x,t)=0,g(t)=\sqrt{\frac{d(\sigma_t^2)}{dt}}\]</span> 于是VE-SDE为： <span class="math display">\[dx=\sqrt{\frac{d(\sigma_t^2)}{dt}}d\omega\]</span></p><h4 id="vp-sde">VP-SDE</h4><p>对于DDPM，扩散公式为： <span class="math display">\[x_T=\sqrt{\bar\alpha_T}x_0+\sqrt{1-\bar\alpha_T}\varepsilon\]</span> 依靠非常小的 <spanclass="math inline">\(\sqrt{\bar\alpha_T}\)</span> 去压制 <spanclass="math inline">\(x_0\)</span> ，而本身方差 <spanclass="math inline">\(\sqrt{1-\bar\alpha_T}\)</span>并不大，因此是Variance Preserving（方差缩紧）。</p><p>DDPM的扰动核 <spanclass="math inline">\(\{p_{\alpha_i}(x|x_0)\}_{i=1}^N\)</span> ,它的离散化的马尔科夫链表示： <span class="math display">\[x_i=\sqrt{1-\beta_i}x_{i-1}+\sqrt{\beta_i}z_{i-1},i=1,2,...,N\]</span> 这里 <span class="math inline">\(z_{i-1}\sim\mathcalN(0,I)\)</span> , 为了获得当 <span class="math inline">\(n\to\infty\)</span> 时的马尔科夫链的极限，定义一个噪声的辅助集合 <spanclass="math inline">\(\{\bar\beta_i=N\beta_i\}_{i=1}^N\)</span> ,于是：<span class="math display">\[x_i=\sqrt{1-\frac{\bar\beta_i}{N}}x_{i-1}+\sqrt{\frac{\bar\beta_i}{N}}z_{i-1},i=1,2,...,N\]</span> 当 <span class="math inline">\(N\to \infty\)</span>时，同上，得： <span class="math display">\[x(t+\Delta t)=\sqrt{1-\beta(t+\Delta t)\Delta t}\x(t)+\sqrt{\beta(t+\Delta t)\Delta t}\ z(t)\\\approx (1-\frac12\beta(t+\Delta t)\Delta t)x(t)+\sqrt{\beta(t+\Deltat)\Delta t}\ z(t)\\\approx (1-\frac12\beta(t)\Delta t)x(t)+\sqrt{\beta(t)\Delta t}\ z(t)\]</span> 于是当 <span class="math inline">\(\Delta t\to 0\)</span> 时：<span class="math display">\[f(x,t)=-\frac12\beta(t)x(t),g(t)=\sqrt{\beta(t)}\]</span> 于是VP-SDE为： <span class="math display">\[dx=-\frac12\beta(t)xdt+\sqrt{\beta(t)}d\omega\]</span></p>]]></content>
    
    
    
    <tags>
      
      <tag>Diffusion</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
